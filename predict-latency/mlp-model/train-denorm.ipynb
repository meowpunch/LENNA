{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------normalize--------------\n",
      " X_Data:  [[0.4        0.52302632 0.10526316 ... 0.75657895 0.10526316 0.28571429]\n",
      " [1.         0.70065789 0.84210526 ... 0.92763158 0.10526316 0.28571429]\n",
      " [1.         0.64802632 0.10526316 ... 0.35855263 0.31578947 1.        ]\n",
      " ...\n",
      " [0.2        0.25986842 0.73684211 ... 0.34868421 0.15789474 0.71428571]\n",
      " [0.4        0.89473684 0.57894737 ... 0.10855263 0.26315789 0.28571429]\n",
      " [1.         0.01315789 0.36842105 ... 0.11184211 0.42105263 0.85714286]]\n",
      "[-1.13230866 -1.12507744 -0.98772211 ...  0.50383923  1.50817799\n",
      "  2.09998707]\n"
     ]
    }
   ],
   "source": [
    "import set_data as sd\n",
    "\n",
    "# load train set, test set\n",
    "\n",
    "X_train, Y_train, X_test, Y_test, y_min, y_max, y_mean, y_std = sd.set_data(\"data/combined_total_data_3\")\n",
    "\n",
    "# print(X_train.shape)\n",
    "# print(X_train, Y_train)\n",
    "# print(Y_train[0])\n",
    "\n",
    "# y_test = Variable(Y_test)\n",
    "# for i in range(20):\n",
    "#     print(y_test)\n",
    "\n",
    "x_train = Variable(X_train)\n",
    "y_train = Variable(Y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regression model\n",
    "\n",
    "class RegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RegressionModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(28, 64) #input dim = 28, hidden = 128\n",
    "        self.drop1 = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(64, 10)\n",
    "        self.drop2 = nn.Dropout(0.2)\n",
    "        self.fc3 = nn.Linear(10,5)\n",
    "        self.drop3 = nn.Dropout(0.1)\n",
    "        self.fc4 = nn.Linear(5, 1)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        a = F.relu(self.fc1(x))\n",
    "        a = self.drop1(a)\n",
    "        a = F.relu(self.fc2(a))\n",
    "        a = self.drop2(a)\n",
    "        a = F.relu(self.fc3(a))\n",
    "        a = self.drop3(a)\n",
    "        a = self.fc4(a)\n",
    "        \n",
    "        return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "\n",
    "\n",
    "\n",
    "loss_graph = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start Training...\n",
      "\n",
      "epoch [0/1000], loss 0.13995100773763808\n",
      "epoch [10/1000], loss 0.1362400060218599\n",
      "epoch [20/1000], loss 0.13358147174520288\n",
      "epoch [30/1000], loss 0.13222548755242616\n",
      "epoch [40/1000], loss 0.13065902637749507\n",
      "epoch [50/1000], loss 0.1291801018142287\n",
      "epoch [60/1000], loss 0.1278709227412643\n",
      "epoch [70/1000], loss 0.12695937392654155\n",
      "epoch [80/1000], loss 0.1261952611650045\n",
      "epoch [90/1000], loss 0.1248123437003774\n",
      "epoch [100/1000], loss 0.1236742959542715\n",
      "epoch [110/1000], loss 0.12284101533840543\n",
      "epoch [120/1000], loss 0.12266116312966589\n",
      "epoch [130/1000], loss 0.1218202143080206\n",
      "epoch [140/1000], loss 0.1217040526391194\n",
      "epoch [150/1000], loss 0.12130125612189778\n",
      "epoch [160/1000], loss 0.12100875280669579\n",
      "epoch [170/1000], loss 0.12091911800673878\n",
      "epoch [180/1000], loss 0.1205693170075592\n",
      "epoch [190/1000], loss 0.12033643079578978\n",
      "epoch [200/1000], loss 0.12040619647799411\n",
      "epoch [210/1000], loss 0.12050850258727153\n",
      "epoch [220/1000], loss 0.12033094555868669\n",
      "epoch [230/1000], loss 0.12010887022877154\n",
      "epoch [240/1000], loss 0.119447300221757\n",
      "epoch [250/1000], loss 0.11996147455779539\n",
      "epoch [260/1000], loss 0.11943847021880452\n",
      "epoch [270/1000], loss 0.11953701435619922\n",
      "epoch [280/1000], loss 0.12009867118891436\n",
      "epoch [290/1000], loss 0.1199536531109306\n",
      "epoch [300/1000], loss 0.11954588214455611\n",
      "epoch [310/1000], loss 0.11929055302616055\n",
      "epoch [320/1000], loss 0.11957824148335762\n",
      "epoch [330/1000], loss 0.12002849238946951\n",
      "epoch [340/1000], loss 0.11961593582928264\n",
      "epoch [350/1000], loss 0.11985134349771279\n",
      "epoch [360/1000], loss 0.11969051036598502\n",
      "epoch [370/1000], loss 0.11943076549357917\n",
      "epoch [380/1000], loss 0.11979470799297547\n",
      "epoch [390/1000], loss 0.11949648123383173\n",
      "epoch [400/1000], loss 0.12010335999249545\n",
      "epoch [410/1000], loss 0.11965866592976444\n",
      "epoch [420/1000], loss 0.11958713284753582\n",
      "epoch [430/1000], loss 0.11906240462598276\n",
      "epoch [440/1000], loss 0.1192296704001109\n",
      "epoch [450/1000], loss 0.11955151000840002\n",
      "epoch [460/1000], loss 0.118742018567424\n",
      "epoch [470/1000], loss 0.1191242922106767\n",
      "epoch [480/1000], loss 0.11867606502403208\n",
      "epoch [490/1000], loss 0.11946020277390365\n",
      "epoch [500/1000], loss 0.11940188384807573\n",
      "epoch [510/1000], loss 0.11888438622949178\n",
      "epoch [520/1000], loss 0.11894926081842828\n",
      "epoch [530/1000], loss 0.11873516409931645\n",
      "epoch [540/1000], loss 0.11908708758704549\n",
      "epoch [550/1000], loss 0.11905685849166123\n",
      "epoch [560/1000], loss 0.11882576429344234\n",
      "epoch [570/1000], loss 0.11867443501948316\n",
      "epoch [580/1000], loss 0.1192588457079028\n",
      "epoch [590/1000], loss 0.11926632996135111\n",
      "epoch [600/1000], loss 0.1188827657780257\n",
      "epoch [610/1000], loss 0.11938417242905099\n",
      "epoch [620/1000], loss 0.11883668854747588\n",
      "epoch [630/1000], loss 0.11855028119882227\n",
      "epoch [640/1000], loss 0.11908040210009294\n",
      "epoch [650/1000], loss 0.11871771584978458\n",
      "epoch [660/1000], loss 0.11867285106265213\n",
      "epoch [670/1000], loss 0.11910876792957988\n",
      "epoch [680/1000], loss 0.11901197956888422\n",
      "epoch [690/1000], loss 0.11899167365939836\n",
      "epoch [700/1000], loss 0.11924976794825394\n",
      "epoch [710/1000], loss 0.1186196882407871\n",
      "epoch [720/1000], loss 0.11871170898543246\n",
      "epoch [730/1000], loss 0.11918968986156711\n",
      "epoch [740/1000], loss 0.11875694565105566\n",
      "epoch [750/1000], loss 0.11884234552461115\n",
      "epoch [760/1000], loss 0.11903358266931181\n",
      "epoch [770/1000], loss 0.1185819679553901\n",
      "epoch [780/1000], loss 0.1186546496652485\n",
      "epoch [790/1000], loss 0.11939986761934356\n",
      "epoch [800/1000], loss 0.1185883083358551\n",
      "epoch [810/1000], loss 0.11869881252034847\n",
      "epoch [820/1000], loss 0.1190007360694603\n",
      "epoch [830/1000], loss 0.11830115048677194\n",
      "epoch [840/1000], loss 0.11898120642778817\n",
      "epoch [850/1000], loss 0.11895958538311865\n",
      "epoch [860/1000], loss 0.1189240090744597\n",
      "epoch [870/1000], loss 0.11878825299790317\n",
      "epoch [880/1000], loss 0.1189335028629082\n",
      "epoch [890/1000], loss 0.11837884648996219\n",
      "epoch [900/1000], loss 0.11918926462318334\n",
      "epoch [910/1000], loss 0.11892842689849667\n",
      "epoch [920/1000], loss 0.11857005273809845\n",
      "epoch [930/1000], loss 0.11849367618987533\n",
      "epoch [940/1000], loss 0.11856261486553388\n",
      "epoch [950/1000], loss 0.11885546165807517\n",
      "epoch [960/1000], loss 0.11877022089298889\n",
      "epoch [970/1000], loss 0.11820693619846549\n",
      "epoch [980/1000], loss 0.11850454932555196\n",
      "epoch [990/1000], loss 0.11851784780570063\n",
      "epoch [1000/1000], loss 0.11849189166038404\n"
     ]
    }
   ],
   "source": [
    "epochs = 1000\n",
    "learningrate = 0.01\n",
    "\n",
    "model = RegressionModel()\n",
    "model.load_state_dict(torch.load('./lenna_d.pth'))\n",
    "criterion = nn.SmoothL1Loss(size_average = True) \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learningrate)\n",
    "\n",
    "print('\\nStart Training...\\n')\n",
    "\n",
    "for epoch in range(epochs+1):\n",
    "    # forward pass\n",
    "    pred_y = model(x_train.float())\n",
    "    \n",
    "\n",
    "    # compute and print loss\n",
    "    loss = criterion(pred_y.double(), y_train)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    loss_graph.append(loss.item())\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print ('epoch [{}/{}], loss {}'.format(epoch, epochs, loss.item())) # or loss item\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"./lenna_d.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot(range(epochs+102), loss_graph, label = 'loss', color = 'black')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.grid(linestyle = '--', color = 'lavender')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss after Training 0.2493415259671085\n"
     ]
    }
   ],
   "source": [
    "x_test = Variable(X_test)\n",
    "y_test = Variable(Y_test)\n",
    "\n",
    "model.eval()\n",
    "y_pred = model(x_test.float())\n",
    "after_train = criterion(y_pred.double(), y_test) \n",
    "print('Test loss after Training' , after_train.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1933],\n",
      "        [ 0.5295],\n",
      "        [-0.2456],\n",
      "        ...,\n",
      "        [ 0.5038],\n",
      "        [ 1.5082],\n",
      "        [ 2.1000]], dtype=torch.float64)\n",
      "Test loss after Training 0.2493415259671085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs/home/meowpunch/.local/lib/python3.6/site-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "x_test = Variable(X_test)\n",
    "y_test = Variable(Y_test)\n",
    "print(y_test)\n",
    "criterion = nn.SmoothL1Loss(size_average = True) \n",
    "\n",
    "new_model= RegressionModel()\n",
    "new_model.load_state_dict(torch.load('./lenna_d.pth'))\n",
    "    \n",
    "new_model.eval()\n",
    "\n",
    "y_pred = new_model(x_test.float())\n",
    "after_train = criterion(y_pred.double(), y_test) \n",
    "print('Test loss after Training' , after_train.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.1933], dtype=torch.float64) tensor([0.1130], grad_fn=<SelectBackward>)\n",
      "tensor([0.5295], dtype=torch.float64) tensor([0.7504], grad_fn=<SelectBackward>)\n",
      "tensor([-0.2456], dtype=torch.float64) tensor([0.1610], grad_fn=<SelectBackward>)\n",
      "tensor([0.0414], dtype=torch.float64) tensor([0.4066], grad_fn=<SelectBackward>)\n",
      "tensor([-0.9015], dtype=torch.float64) tensor([-0.4868], grad_fn=<SelectBackward>)\n",
      "tensor([-0.4572], dtype=torch.float64) tensor([-0.1539], grad_fn=<SelectBackward>)\n",
      "tensor([-0.1019], dtype=torch.float64) tensor([0.0220], grad_fn=<SelectBackward>)\n",
      "tensor([1.5011], dtype=torch.float64) tensor([0.7785], grad_fn=<SelectBackward>)\n",
      "tensor([-0.4985], dtype=torch.float64) tensor([-0.2176], grad_fn=<SelectBackward>)\n",
      "tensor([0.8036], dtype=torch.float64) tensor([0.9932], grad_fn=<SelectBackward>)\n",
      "tensor([1.1582], dtype=torch.float64) tensor([1.2911], grad_fn=<SelectBackward>)\n",
      "tensor([-0.1585], dtype=torch.float64) tensor([-0.2244], grad_fn=<SelectBackward>)\n",
      "tensor([-0.5299], dtype=torch.float64) tensor([-0.4061], grad_fn=<SelectBackward>)\n",
      "tensor([-0.3164], dtype=torch.float64) tensor([-0.4030], grad_fn=<SelectBackward>)\n",
      "tensor([0.1469], dtype=torch.float64) tensor([-0.0681], grad_fn=<SelectBackward>)\n",
      "tensor([0.6266], dtype=torch.float64) tensor([0.5761], grad_fn=<SelectBackward>)\n",
      "tensor([-0.3047], dtype=torch.float64) tensor([-0.3495], grad_fn=<SelectBackward>)\n",
      "tensor([-0.8124], dtype=torch.float64) tensor([-0.3725], grad_fn=<SelectBackward>)\n",
      "tensor([-2.2230], dtype=torch.float64) tensor([-1.3405], grad_fn=<SelectBackward>)\n",
      "tensor([-1.4274], dtype=torch.float64) tensor([-0.8944], grad_fn=<SelectBackward>)\n",
      "tensor([173599.0938]) tensor([189468.0469])\n",
      "tensor([211044.1406]) tensor([222484.9844])\n",
      "tensor([170892.2031]) tensor([191953.1406])\n",
      "tensor([185761.7188]) tensor([204677.1094])\n",
      "tensor([136916.3906]) tensor([158396.9844])\n",
      "tensor([159931.5781]) tensor([175641.1875])\n",
      "tensor([178334.2500]) tensor([184753.9844])\n",
      "tensor([261374.8281]) tensor([223941.7188])\n",
      "tensor([157791.0938]) tensor([172339.9219])\n",
      "tensor([225242.0938]) tensor([235063.0625])\n",
      "tensor([243611.4062]) tensor([250497.5156])\n",
      "tensor([175406.0938]) tensor([171990.5312])\n",
      "tensor([156165.0938]) tensor([162578.6875])\n",
      "tensor([167223.8438]) tensor([162735.5938])\n",
      "tensor([191224.5469]) tensor([180085.0625])\n",
      "tensor([216072.8125]) tensor([213457.2500])\n",
      "tensor([167831.0781]) tensor([165508.8906])\n",
      "tensor([141528.6562]) tensor([164315.8438])\n",
      "tensor([68457.1328]) tensor([114173.2266])\n",
      "tensor([109672.3281]) tensor([137284.4062])\n"
     ]
    }
   ],
   "source": [
    "def denormalize(data, d_min, d_max, d_mean, d_std):\n",
    "    \n",
    "    tmp_data = torch.empty([data.shape[0], data.shape[1]])\n",
    "    \n",
    "    for i in range(data.shape[0]):\n",
    "        e = float(data[i])\n",
    "        \n",
    "        #tmp_data[i] = (e*d_std + d_mean)\n",
    "        tmp_data[i] = (e*d_std + d_mean)*(d_max - d_min) + d_min\n",
    "\n",
    "    return tmp_data\n",
    "\n",
    "for i in range(20):\n",
    "    print(y_test[i], y_pred[i])\n",
    "# print(float(y_pred[0]))\n",
    "# print(y_test.float())\n",
    "# print(y_pred[0])\n",
    "\n",
    "dy_test = denormalize(y_test, y_min, y_max, y_mean, y_std)\n",
    "dy_pred = denormalize(y_pred, y_min, y_max, y_mean, y_std)\n",
    "\n",
    "for i in range(20):\n",
    "    print(dy_test[i], dy_pred[i])\n",
    "# print(dy_test)\n",
    "# print(dy_pred)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LENNA!",
   "language": "python",
   "name": "lenna"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
