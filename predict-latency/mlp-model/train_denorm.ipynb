{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data as Data\n",
    "\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
<<<<<<< Updated upstream
=======
   "outputs": [],
   "source": [
    "class RegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RegressionModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(28, 1) #input dim = 28, hidden = 128\n",
    "        # self.fc2 = nn.Linear(10, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.sigmoid(self.fc1(x))\n",
    "        # x = self.fc2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
>>>>>>> Stashed changes
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------X data--------------\n",
      " [[  3. 110.  12. ...  28.   1.   3.]\n",
      " [  5. 191.   5. ... 307.   2.   6.]\n",
      " [  2. 272.   4. ... 287.  14.   7.]\n",
      " ...\n",
      " [  2.  95.  15. ... 122.   4.   6.]\n",
      " [  3. 288.  12. ...  49.   6.   3.]\n",
      " [  6.  20.   8. ...  50.   9.   7.]]\n",
      "--------------Y data--------------\n",
      " [111839.182   100069.52175 184629.05875 ... 209714.78025 261742.30675\n",
      " 292399.655  ]\n",
      "tensor([-0.2904, -0.6558,  0.2619,  1.5265, -0.8838,  1.3313,  0.9538,  0.2107,\n",
      "         0.8753, -0.3144, -1.4738, -1.0943, -0.8747, -0.0122, -1.2993,  1.5285,\n",
      "        -1.4652, -0.9095, -0.9493,  0.2172,  0.8782, -0.4555,  0.9523,  0.2168,\n",
      "         0.8789, -1.5887, -1.6440, -0.6523], dtype=torch.float64) tensor([111839.1820], dtype=torch.float64)\n",
      "tensor([[-0.2904, -0.6558,  0.2619,  ..., -1.5887, -1.6440, -0.6523],\n",
      "        [ 0.8795,  0.2651, -0.9515,  ...,  1.5792, -1.4707,  0.6581],\n",
      "        [-0.8754,  1.1861, -1.1248,  ...,  1.3521,  0.6088,  1.0948],\n",
      "        ...,\n",
      "        [-0.8754, -1.0197, -0.4314,  ...,  0.2961, -0.4310, -0.2155],\n",
      "        [ 0.8795,  0.2879, -1.2982,  ..., -0.7939, -0.0844,  0.2213],\n",
      "        [ 0.8795, -1.4859,  0.7820,  ..., -0.1921,  0.9554, -1.0890]],\n",
      "       dtype=torch.float64)\n",
      "tensor([[111839.1820],\n",
      "        [100069.5218],\n",
      "        [184629.0587],\n",
      "        ...,\n",
      "        [162362.1258],\n",
      "        [189083.0120],\n",
      "        [194573.4770]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import set_data as sd\n",
    "\n",
    "# load train set, test set\n",
    "\n",
    "X_train, Y_train, X_test, Y_test = sd.set_data(\"data/combined_total_data_1\")\n",
    "print(X_train[0], Y_train[0])\n",
    "\n",
    "\n",
    "x_train = Variable(X_train)\n",
    "y_train = Variable(Y_train)\n",
    "\n",
    "print(x_train)\n",
    "print(y_train)\n",
    "\n",
    "x_test = Variable(X_test)\n",
    "y_test = Variable(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------X data--------------\n",
      " [[  3. 110.  12. ...  28.   1.   3.]\n",
      " [  5. 191.   5. ... 307.   2.   6.]\n",
      " [  2. 272.   4. ... 287.  14.   7.]\n",
      " ...\n",
      " [  2.  95.  15. ... 122.   4.   6.]\n",
      " [  3. 288.  12. ...  49.   6.   3.]\n",
      " [  6.  20.   8. ...  50.   9.   7.]]\n",
      "--------------Y data--------------\n",
      " [111839.182   100069.52175 184629.05875 ... 209714.78025 261742.30675\n",
      " 292399.655  ]\n",
      "tensor([0.3333, 0.2938, 0.5500, 0.8750, 0.1667, 0.8406, 0.7500, 0.5000, 0.6667,\n",
      "        0.3875, 0.0500, 0.1250, 0.1667, 0.4719, 0.1000, 0.8750, 0.0000, 0.2250,\n",
      "        0.2000, 0.5000, 0.6667, 0.3500, 0.7500, 0.5000, 0.6667, 0.0375, 0.0000,\n",
      "        0.2500], dtype=torch.float64) tensor([111839.1820], dtype=torch.float64)\n",
      "epoch [0/200], loss 790381018.2848125\n",
      "epoch [1/200], loss 909696914.8680139\n",
      "epoch [2/200], loss 325611204.8170671\n",
      "epoch [3/200], loss 795108875.7017833\n",
      "epoch [4/200], loss 357642786.458227\n",
      "epoch [5/200], loss 445847373.7537315\n",
      "epoch [6/200], loss 475896169.7099056\n",
      "epoch [7/200], loss 1394254992.6660025\n",
      "epoch [8/200], loss 829720338.9533159\n",
      "epoch [9/200], loss 2265763692.6205945\n",
      "epoch [10/200], loss 392305829.28418595\n",
      "epoch [11/200], loss 494592304.88667315\n",
      "epoch [12/200], loss 454503165.9258378\n",
      "epoch [13/200], loss 667743888.640893\n",
      "epoch [14/200], loss 982490098.6903626\n",
      "epoch [15/200], loss 427287507.0611606\n",
      "epoch [16/200], loss 687201548.0934979\n",
      "epoch [17/200], loss 527414870.12768096\n",
      "epoch [18/200], loss 389616705.54368687\n",
      "epoch [19/200], loss 429942160.85760516\n",
      "epoch [20/200], loss 368576232.7594926\n",
      "epoch [21/200], loss 3258042090.19638\n",
      "epoch [22/200], loss 599105936.8641014\n",
      "epoch [23/200], loss 953748640.8000969\n",
      "epoch [24/200], loss 1798116874.6735926\n",
      "epoch [25/200], loss 2027704060.7610939\n",
      "epoch [26/200], loss 1662729626.9859896\n",
      "epoch [27/200], loss 1267472160.0775049\n",
      "epoch [28/200], loss 532062482.68963885\n",
      "epoch [29/200], loss 2151152289.986085\n",
      "epoch [30/200], loss 516219372.8672817\n",
      "epoch [31/200], loss 1776639257.743779\n",
      "epoch [32/200], loss 727899403.8619944\n",
      "epoch [33/200], loss 903745312.7826519\n",
      "epoch [34/200], loss 681354227.9254458\n",
      "epoch [35/200], loss 841810001.0461417\n",
      "epoch [36/200], loss 1474128303.6628568\n",
      "epoch [37/200], loss 452768401.6051051\n",
      "epoch [38/200], loss 298140239.36314833\n",
      "epoch [39/200], loss 556409503.4570477\n",
      "epoch [40/200], loss 277427526.9682577\n",
      "epoch [41/200], loss 420994824.86292267\n",
      "epoch [42/200], loss 523974939.8478241\n",
      "epoch [43/200], loss 709566667.8359011\n",
      "epoch [44/200], loss 558681246.055864\n",
      "epoch [45/200], loss 1023633815.8374267\n",
      "epoch [46/200], loss 395211000.48798037\n",
      "epoch [47/200], loss 410021662.0647264\n",
      "epoch [48/200], loss 442095822.2820735\n",
      "epoch [49/200], loss 661953262.1620218\n",
      "epoch [50/200], loss 633130975.3771771\n",
      "epoch [51/200], loss 463529871.18035316\n",
      "epoch [52/200], loss 312342562.50001866\n",
      "epoch [53/200], loss 609696799.104704\n",
      "epoch [54/200], loss 426749070.13217324\n",
      "epoch [55/200], loss 239613671.5700382\n",
      "epoch [56/200], loss 444925771.9712241\n",
      "epoch [57/200], loss 747185090.1847483\n",
      "epoch [58/200], loss 398507971.4628777\n",
      "epoch [59/200], loss 413905797.46611935\n",
      "epoch [60/200], loss 546433480.2751647\n",
      "epoch [61/200], loss 283222498.18439704\n",
      "epoch [62/200], loss 335720549.5527348\n",
      "epoch [63/200], loss 1066526745.6641521\n",
      "epoch [64/200], loss 913622180.9700308\n",
      "epoch [65/200], loss 670412156.8129455\n",
      "epoch [66/200], loss 1222699739.6599247\n",
      "epoch [67/200], loss 176861728.81380385\n",
      "epoch [68/200], loss 547186557.4216162\n",
      "epoch [69/200], loss 988897010.889192\n",
      "epoch [70/200], loss 369369468.47470385\n",
      "epoch [71/200], loss 315266848.539712\n",
      "epoch [72/200], loss 378915697.91740507\n",
      "epoch [73/200], loss 899164187.4992546\n",
      "epoch [74/200], loss 641273204.2744416\n",
      "epoch [75/200], loss 403805127.07998055\n",
      "epoch [76/200], loss 264855507.92578876\n",
      "epoch [77/200], loss 341419592.9155542\n",
      "epoch [78/200], loss 543633386.5669461\n",
      "epoch [79/200], loss 373809257.4671187\n",
      "epoch [80/200], loss 219126559.0806513\n",
      "epoch [81/200], loss 401598360.4533384\n",
      "epoch [82/200], loss 2273418513.93177\n",
      "epoch [83/200], loss 1075414235.868111\n",
      "epoch [84/200], loss 240348939.17545104\n",
      "epoch [85/200], loss 211744338.85125637\n",
      "epoch [86/200], loss 417004962.3533686\n",
      "epoch [87/200], loss 346859381.919027\n",
      "epoch [88/200], loss 499372807.5845216\n",
      "epoch [89/200], loss 908871778.678599\n",
      "epoch [90/200], loss 261586962.45770955\n",
      "epoch [91/200], loss 226611421.22319612\n",
      "epoch [92/200], loss 575622868.2228917\n",
      "epoch [93/200], loss 180306267.56129003\n",
      "epoch [94/200], loss 770967703.4736853\n",
      "epoch [95/200], loss 363658628.11262745\n",
      "epoch [96/200], loss 641147384.1160207\n",
      "epoch [97/200], loss 267619199.8246759\n",
      "epoch [98/200], loss 1011120938.6259662\n",
      "epoch [99/200], loss 416783766.72161776\n",
      "epoch [100/200], loss 2852492215.9112372\n",
      "epoch [101/200], loss 504104253.7112934\n",
      "epoch [102/200], loss 775378424.102366\n",
      "epoch [103/200], loss 128456878.5989901\n",
      "epoch [104/200], loss 587244934.2089161\n",
      "epoch [105/200], loss 518261284.9872492\n",
      "epoch [106/200], loss 678877904.421683\n",
      "epoch [107/200], loss 376082266.30288815\n",
      "epoch [108/200], loss 388100508.19734794\n",
      "epoch [109/200], loss 141085598.35400575\n",
      "epoch [110/200], loss 294525707.6310857\n",
      "epoch [111/200], loss 195494201.40898445\n",
      "epoch [112/200], loss 596048728.7570316\n",
      "epoch [113/200], loss 371060615.00032467\n",
      "epoch [114/200], loss 388233912.9025077\n",
      "epoch [115/200], loss 560321530.4392315\n",
      "epoch [116/200], loss 310259004.1595548\n",
      "epoch [117/200], loss 374845864.48169494\n",
      "epoch [118/200], loss 305514990.57253736\n",
      "epoch [119/200], loss 1586568410.758118\n",
      "epoch [120/200], loss 329091135.97900647\n",
      "epoch [121/200], loss 355820762.9445389\n",
      "epoch [122/200], loss 476135844.6289657\n",
      "epoch [123/200], loss 979857918.7727655\n",
      "epoch [124/200], loss 369383934.4093899\n",
      "epoch [125/200], loss 371676564.1440857\n",
      "epoch [126/200], loss 550127810.1769127\n",
      "epoch [127/200], loss 182141121.2769979\n",
      "epoch [128/200], loss 474840996.2176547\n",
      "epoch [129/200], loss 567442172.1596984\n",
      "epoch [130/200], loss 1273134927.6636353\n",
      "epoch [131/200], loss 1182998627.6984918\n",
      "epoch [132/200], loss 799861829.1163279\n",
      "epoch [133/200], loss 402298431.6078613\n",
      "epoch [134/200], loss 513121004.0034017\n",
      "epoch [135/200], loss 152665553.7164931\n",
      "epoch [136/200], loss 447458647.0262387\n",
      "epoch [137/200], loss 332807974.7364479\n",
      "epoch [138/200], loss 170603979.53723577\n",
      "epoch [139/200], loss 621799105.1587176\n",
      "epoch [140/200], loss 325910815.52922404\n",
      "epoch [141/200], loss 382563326.55814767\n",
      "epoch [142/200], loss 178026869.93222582\n",
      "epoch [143/200], loss 370090677.8605446\n",
      "epoch [144/200], loss 1151396584.763943\n",
      "epoch [145/200], loss 220213498.73516563\n",
      "epoch [146/200], loss 177364990.60236454\n",
      "epoch [147/200], loss 633526238.339883\n",
      "epoch [148/200], loss 214245212.0862484\n",
      "epoch [149/200], loss 253312209.4519972\n",
      "epoch [150/200], loss 551371211.0088398\n",
      "epoch [151/200], loss 973957459.2919284\n",
      "epoch [152/200], loss 766051617.834925\n",
      "epoch [153/200], loss 282415455.38310486\n",
      "epoch [154/200], loss 242781469.74641705\n",
      "epoch [155/200], loss 1710065249.7613878\n",
      "epoch [156/200], loss 228525794.45747218\n",
      "epoch [157/200], loss 1337396900.269485\n",
      "epoch [158/200], loss 311416343.08504766\n",
      "epoch [159/200], loss 375451450.2296497\n",
      "epoch [160/200], loss 531389078.8702812\n",
      "epoch [161/200], loss 311365738.7035136\n",
      "epoch [162/200], loss 470292938.7386253\n",
      "epoch [163/200], loss 598129066.0587522\n",
      "epoch [164/200], loss 165716669.85236505\n",
      "epoch [165/200], loss 266174301.8999326\n",
      "epoch [166/200], loss 289927355.72967654\n",
      "epoch [167/200], loss 226011114.8968881\n",
      "epoch [168/200], loss 441595633.0058658\n",
      "epoch [169/200], loss 230472927.69745386\n",
      "epoch [170/200], loss 176595606.87346193\n",
      "epoch [171/200], loss 157407253.79424617\n",
      "epoch [172/200], loss 1259240259.6091235\n",
      "epoch [173/200], loss 277508401.9827774\n",
      "epoch [174/200], loss 296846920.6648617\n",
      "epoch [175/200], loss 555713832.4198769\n",
      "epoch [176/200], loss 325590445.1295044\n",
      "epoch [177/200], loss 345941127.5456683\n",
      "epoch [178/200], loss 540777492.0578606\n",
      "epoch [179/200], loss 337504857.50922745\n",
      "epoch [180/200], loss 302657691.4553855\n",
      "epoch [181/200], loss 225930921.4009981\n",
      "epoch [182/200], loss 243534336.2407659\n",
      "epoch [183/200], loss 253980847.2301753\n",
      "epoch [184/200], loss 1649702190.8352005\n",
      "epoch [185/200], loss 194610826.1963769\n",
      "epoch [186/200], loss 650720379.1428419\n",
      "epoch [187/200], loss 508580761.1650358\n",
      "epoch [188/200], loss 137902195.65587083\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [189/200], loss 764783337.8550663\n",
      "epoch [190/200], loss 346629341.6739734\n",
      "epoch [191/200], loss 324172284.8475601\n",
      "epoch [192/200], loss 347085745.6886523\n",
      "epoch [193/200], loss 1115097048.6342638\n",
      "epoch [194/200], loss 608876830.8868235\n",
      "epoch [195/200], loss 479686877.244551\n",
      "epoch [196/200], loss 272980067.8146293\n",
      "epoch [197/200], loss 315695062.6281773\n",
      "epoch [198/200], loss 226493113.71805438\n",
      "epoch [199/200], loss 2524813942.1576824\n"
     ]
    }
   ],
   "source": [
    "# import set_data as sd\n",
    "\n",
    "# # load train set, test set\n",
    "\n",
    "# X_train, Y_train, X_test, Y_test = sd.set_data(\"data/combined_total_data_1\")\n",
    "# print(X_train[0], Y_train[0])\n",
    "\n",
    "# model = torch.nn.Sequential(\n",
    "#         torch.nn.Linear(28, 200),\n",
    "#         torch.nn.LeakyReLU(),\n",
    "#         torch.nn.Linear(200, 100),\n",
    "#         torch.nn.LeakyReLU(),\n",
    "#         torch.nn.Linear(100, 1),\n",
    "#     )\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "# loss_func = torch.nn.MSELoss()  # this is for regression mean squared loss\n",
    "\n",
    "# BATCH_SIZE = 64\n",
    "# EPOCH = 200\n",
    "\n",
    "# torch_dataset = Data.TensorDataset(X_train, Y_train)\n",
    "\n",
    "# loader = Data.DataLoader(\n",
    "#     dataset=torch_dataset, \n",
    "#     batch_size=BATCH_SIZE, \n",
    "#     shuffle=True, num_workers=2,)\n",
    "\n",
    "# # x_train = Variable(X_train)\n",
    "# # y_train = Variable(Y_train)\n",
    "\n",
    "# # print(x_train)\n",
    "# # print(y_train)\n",
    "\n",
    "# # x_test = Variable(X_test)\n",
    "# # y_test = Variable(Y_test)\n",
    "\n",
    "# loss_graph = []\n",
    "\n",
    "# for epoch in range(EPOCH):\n",
    "#     for step, (batch_x, batch_y) in enumerate(loader): # for each training step\n",
    "        \n",
    "#         b_x = Variable(batch_x)\n",
    "#         b_y = Variable(batch_y)\n",
    "        \n",
    "#         # print(b_x,b_y)\n",
    "\n",
    "#         prediction = model(b_x.float())     # input x and predict based on x\n",
    "\n",
    "#         loss = loss_func(prediction.double(), b_y)     # must be (1. nn output, 2. target)\n",
    "\n",
    "#         optimizer.zero_grad()   # clear gradients for next train\n",
    "#         loss.backward()         # backpropagation, compute gradients\n",
    "#         optimizer.step()        # apply gradients\n",
    "        \n",
    "#         loss_graph.append(loss.item())\n",
    "        \n",
    "    \n",
    "   \n",
    "#     print('epoch [{}/{}], loss {}'.format(epoch, EPOCH, loss.item())) # or loss item\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 4,
=======
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2479.02857232  3803.01688743 16779.11166652 -9069.57976799\n",
      "    727.58482971   984.22271371 14035.01141568 -1821.23970995\n",
      "    468.36535919   581.05958694 13487.32628884  -456.69258545\n",
      "    279.29802044   388.7119003  13271.79181684  -312.95656165\n",
      "    264.54764752   296.95914332 13141.7040825   -186.54333982\n",
      "    164.00358966   319.3927282  13255.2020768    -97.93123438\n",
      "    427.45863475   450.40197838 13240.6643495   -114.76443697]]\n",
      "tensor([[109579.6950],\n",
      "        [243648.9815],\n",
      "        [257048.4855],\n",
      "        ...,\n",
      "        [209714.7802],\n",
      "        [261742.3068],\n",
      "        [292399.6550]], dtype=torch.float64) [[113377.45989085]\n",
      " [264088.00300055]\n",
      " [238257.90031819]\n",
      " ...\n",
      " [136607.58948769]\n",
      " [176370.61223967]\n",
      " [192600.59612319]]\n",
      "0.09899045721786659\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "x_train = Variable(X_train)\n",
    "y_train = Variable(Y_train)\n",
    "\n",
    "# int(x_train)\n",
    "# int(y_train)\n",
    "\n",
    "x_test = Variable(X_test)\n",
    "y_test = Variable(Y_test)\n",
    "\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(x_train, y_train)\n",
    "print(regressor.coef_)\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred = regressor.predict(x_test)\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "score=r2_score(y_test,y_pred)\n",
    "print(y_test,y_pred)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [],
   "source": [
    "# regression model\n",
    "\n",
    "class RegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RegressionModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(28, 64) #input dim = 28, hidden = 128\n",
    "        self.drop1 = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(64, 10)\n",
    "        self.drop2 = nn.Dropout(0.2)\n",
    "        self.fc3 = nn.Linear(10, 5)\n",
    "        self.drop3 = nn.Dropout(0.1)\n",
    "        self.fc4 = nn.Linear(5, 1)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        a = F.relu(self.fc1(x))\n",
    "        a = self.drop1(a)\n",
    "        a = F.relu(self.fc2(a))\n",
    "        a = self.drop2(a)\n",
    "        a = F.relu(self.fc3(a))\n",
    "        a = self.drop3(a)\n",
    "        a = self.fc4(a)\n",
    "        \n",
    "        return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
<<<<<<< Updated upstream
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start Training...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs/home/kit7777777t/anaconda3/envs/LENNA/lib/python3.6/site-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [0/30], loss 0.16219501739827963\n",
      "epoch [10/30], loss 0.047118534998091315\n",
      "epoch [20/30], loss 0.03874450603897746\n",
      "epoch [30/30], loss 0.033846772647649584\n"
=======
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 (0, Parameter containing:\n",
      "tensor([[ 0.1060, -0.1765,  0.0477, -0.1512, -0.1288, -0.1514,  0.1742, -0.0946,\n",
      "         -0.1207,  0.0308,  0.1642,  0.0234, -0.1034, -0.0348, -0.1422,  0.0332,\n",
      "          0.0048,  0.0854, -0.0629,  0.0603,  0.0502,  0.1123, -0.1122, -0.1828,\n",
      "          0.0817, -0.0200, -0.0267, -0.0890]], requires_grad=True))\n",
      "[[ 2479.02857232  3803.01688743 16779.11166652 -9069.57976799\n",
      "    727.58482971   984.22271371 14035.01141568 -1821.23970995\n",
      "    468.36535919   581.05958694 13487.32628884  -456.69258545\n",
      "    279.29802044   388.7119003  13271.79181684  -312.95656165\n",
      "    264.54764752   296.95914332 13141.7040825   -186.54333982\n",
      "    164.00358966   319.3927282  13255.2020768    -97.93123438\n",
      "    427.45863475   450.40197838 13240.6643495   -114.76443697]]\n",
      "1 (1, Parameter containing:\n",
      "tensor([0.1844], requires_grad=True))\n",
      "[[ 2479.02857232  3803.01688743 16779.11166652 -9069.57976799\n",
      "    727.58482971   984.22271371 14035.01141568 -1821.23970995\n",
      "    468.36535919   581.05958694 13487.32628884  -456.69258545\n",
      "    279.29802044   388.7119003  13271.79181684  -312.95656165\n",
      "    264.54764752   296.95914332 13141.7040825   -186.54333982\n",
      "    164.00358966   319.3927282  13255.2020768    -97.93123438\n",
      "    427.45863475   450.40197838 13240.6643495   -114.76443697]]\n",
      "haha\n",
      "<generator object Module.parameters at 0x7efb49878f10>\n",
      "\n",
      "Start Training...\n",
      "\n",
      "epoch [0/500], loss 34700909280.67907\n",
      "epoch [100/500], loss 34700830846.37501\n",
      "epoch [200/500], loss 34700790812.34432\n",
      "epoch [300/500], loss 34700772805.86273\n",
      "epoch [400/500], loss 34700763849.82684\n",
      "epoch [500/500], loss 34700758808.36348\n"
>>>>>>> Stashed changes
     ]
    }
   ],
   "source": [
<<<<<<< Updated upstream
    "# train\n",
    "\n",
    "epochs = 30\n",
    "learningrate = 0.01\n",
    "\n",
    "model = RegressionModel()\n",
    "criterion = nn.SmoothL1Loss(size_average = True) \n",
=======
    "epochs = 500\n",
    "learningrate = 0.01\n",
    "\n",
    "# batch_size\n",
    "# hidden_layer_size = 128\n",
    "\n",
    "model = RegressionModel()\n",
    "# print(model.parameters().shape)\n",
    "for e in enumerate(model.parameters()):\n",
    "    print(idx, e)\n",
    "    e = regressor.coef_\n",
    "    print(e)\n",
    "    \n",
    "print(\"haha\")\n",
    "print(model.parameters())\n",
    "criterion = nn.MSELoss(size_average = True, reduction='mean') \n",
>>>>>>> Stashed changes
    "optimizer = torch.optim.Adam(model.parameters(), lr=learningrate)\n",
    "\n",
    "print('\\nStart Training...\\n')\n",
    "\n",
    "loss_graph = []\n",
    "for epoch in range(epochs+1):\n",
    "    # forward pass\n",
    "    pred_y = model(x_train.float())\n",
    "    \n",
    "\n",
    "    # compute and print loss\n",
    "    loss = criterion(pred_y.double(), y_train)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    loss_graph.append(loss.item())\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print ('epoch [{}/{}], loss {}'.format(epoch, epochs, loss.item())) # or loss item\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"./lenna0.pth\")"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 7,
=======
   "execution_count": 13,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (200,) and (273200,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-ba68f4dea908>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPOCH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'black'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'epochs'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinestyle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'--'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'lavender'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/LENNA/lib/python3.6/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2794\u001b[0m     return gca().plot(\n\u001b[1;32m   2795\u001b[0m         *args, scalex=scalex, scaley=scaley, **({\"data\": data} if data\n\u001b[0;32m-> 2796\u001b[0;31m         is not None else {}), **kwargs)\n\u001b[0m\u001b[1;32m   2797\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2798\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/LENNA/lib/python3.6/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1663\u001b[0m         \"\"\"\n\u001b[1;32m   1664\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_alias_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1665\u001b[0;31m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1666\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1667\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/LENNA/lib/python3.6/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    223\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_next_color\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/LENNA/lib/python3.6/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[0;34m(self, tup, kwargs)\u001b[0m\n\u001b[1;32m    389\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex_of\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 391\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_xy_from_xy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommand\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'plot'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/LENNA/lib/python3.6/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_xy_from_xy\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m             raise ValueError(\"x and y must have same first dimension, but \"\n\u001b[0;32m--> 270\u001b[0;31m                              \"have shapes {} and {}\".format(x.shape, y.shape))\n\u001b[0m\u001b[1;32m    271\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m             raise ValueError(\"x and y can be no greater than 2-D, but have \"\n",
      "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (200,) and (273200,)"
     ]
    },
    {
     "data": {
<<<<<<< Updated upstream
      "image/png": "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\n",
=======
      "image/png": "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\n",
>>>>>>> Stashed changes
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot(range(EPOCH), loss_graph, label = 'loss', color = 'black')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.grid(linestyle = '--', color = 'lavender')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 8,
=======
   "execution_count": 32,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< Updated upstream
      "Test loss after Training 0.0964340327909631\n"
     ]
    }
   ],
   "source": [
=======
      "tensor([[179480.7969],\n",
      "        [179480.7969],\n",
      "        [179480.7969],\n",
      "        ...,\n",
      "        [179480.7969],\n",
      "        [179480.7969],\n",
      "        [179480.7969]], grad_fn=<AddmmBackward>)\n",
      "torch.Size([21846, 1])\n",
      "torch.Size([21846, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs/home/meowpunch/.local/lib/python3.6/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([21846, 1])) that is different to the input size (torch.Size([21846])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([179480.7969, 179480.7969, 179480.7969,  ..., 179480.7969,\n",
      "        179480.7969, 179480.7969], grad_fn=<SqueezeBackward0>)\n",
      "torch.Size([21846])\n",
      "Test loss after Training 5487857296.436673\n"
     ]
    }
   ],
   "source": [
    "# model=torch.load('lenna0.pth')\n",
>>>>>>> Stashed changes
    "x_test = Variable(X_test)\n",
    "y_test = Variable(Y_test)\n",
    "\n",
    "model.eval()\n",
    "y_pred = model(x_test.float())\n",
<<<<<<< Updated upstream
    "after_train = criterion(y_pred.double(), y_test) \n",
    "print('Test loss after Training' , after_train.item())"
=======
    "\n",
    "print(y_pred)\n",
    "print(y_pred.shape)\n",
    "print(y_test.shape)\n",
    "after_train = criterion(y_pred.squeeze(), y_test)\n",
    "print(y_pred.squeeze())\n",
    "print(y_pred.squeeze().shape)\n",
    "print('Test loss after Training' , after_train.item())\n",
    "\n",
    "\n"
>>>>>>> Stashed changes
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RegressionModel(\n",
       "  (fc1): Linear(in_features=28, out_features=64, bias=True)\n",
       "  (drop1): Dropout(p=0.5, inplace=False)\n",
       "  (fc2): Linear(in_features=64, out_features=10, bias=True)\n",
       "  (drop2): Dropout(p=0.2, inplace=False)\n",
       "  (fc3): Linear(in_features=10, out_features=5, bias=True)\n",
       "  (drop3): Dropout(p=0.1, inplace=False)\n",
       "  (fc4): Linear(in_features=5, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model= RegressionModel()\n",
    "new_model.load_state_dict(torch.load('./lenna0.pth'))\n",
    "    \n",
    "new_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LENNA!",
   "language": "python",
   "name": "lenna"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
